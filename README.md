# Vision Transformers are Parameter-Efficient Audio-Visual Learners

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) <img src="https://raw.githubusercontent.com/facebookresearch/unbiased-teacher/main/teaser/pytorch-logo-dark.png" width="10%"> 
<!-- [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) -->

This is the PyTorch implementation of our paper: <br>
**Vision Transformers are Parameter-Efficient Audio-Visual Learners**<br>
[Yan-Bo Lin](https://genjib.github.io/), [Yi-Lin Sung](https://ylsung.github.io/),[Jie Lei](https://jayleicn.github.io/), [Mohit Bansal](https://www.cs.unc.edu/~mbansal/), and [Gedas Bertasius](https://www.gedasbertasius.com/)<br>
<br>

[Paper](https://arxiv.org/abs/2204.02874) [Project Page](https://genjib.github.io/project_page/LAVISH/) 

### üìù Preparation 
*  `See each foloder for more detailed settings`
* Audio-Visual Event Localization: ./AVE
* Audio-Visual Segmentation: ./AVS
* Audio-Visual Question Answering: ./AVQA

### üéì Cite

If you use this code in your research, please cite:

```bibtex
@InProceedings{LAVISH_arxiv2022,
author = {Yan-Bo Lin and Yi-Lin Sung and Jie Lei and Mohit Bansal and Gedas Bertasius},
title = {Vision Transformers are Parameter-Efficient Audio-Visual Learners},
booktitle = {arXiv},
year = {2022}
}
```

### üëç Acknowledgments
Our code is based on [AVSBench](https://github.com/OpenNLPLab/AVSBench) and [MUSIC-AVQA](https://github.com/GeWu-Lab/MUSIC-AVQA)
