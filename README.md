# Vision Transformers are Parameter-Efficient Audio-Visual Learners

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) <img src="https://raw.githubusercontent.com/facebookresearch/unbiased-teacher/main/teaser/pytorch-logo-dark.png" width="10%"> 

<p align="center">
<img src="https://genjib.github.io/project_page/LAVISH/assets/teaser.png" width="50%">
</p align="center">

This is the PyTorch implementation of our paper: <br>
**Vision Transformers are Parameter-Efficient Audio-Visual Learners**<br>
[Yan-Bo Lin](https://genjib.github.io/), [Yi-Lin Sung](https://ylsung.github.io/), [Jie Lei](https://jayleicn.github.io/), [Mohit Bansal](https://www.cs.unc.edu/~mbansal/), and [Gedas Bertasius](https://www.gedasbertasius.com/)<br>
<br>

<br>**Our Method**<br>

<p align="center">
<img src="https://genjib.github.io/project_page/LAVISH/assets/method.png" width="70%">
</p align="center">

[ğŸ“—Paper](https://arxiv.org/abs/2212.07983) [ğŸ Project Page](https://genjib.github.io/project_page/LAVISH/) 

### ğŸ“ Preparation 
*  `See each folder for more detailed settings`
* Audio-Visual Event Localization: ./AVE
* Audio-Visual Segmentation: ./AVS
* Audio-Visual Question Answering: ./AVQA

### ğŸ“ Cite

If you use this code in your research, please cite:

```bibtex
@InProceedings{LAVISH_arxiv2022,
author = {Lin, Yan-Bo an Sung, Yi-Lin and Lei, Jie and Bansal, Mohit and Bertasius, Gedas},
title = {Vision Transformers are Parameter-Efficient Audio-Visual Learners},
booktitle = {arXiv},
year = {2022}
}
```

### ğŸ‘ Acknowledgments
Our code is based on [AVSBench](https://github.com/OpenNLPLab/AVSBench) and [MUSIC-AVQA](https://github.com/GeWu-Lab/MUSIC-AVQA)
